{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### In this notebook we are going to do data cleaning and exploration. As the original news category dataset is large (200k records).\n### we will filter the data set and optimize it for our NLP implementation\n### Hopefully it will help fellow beginners in getting started with data cleaning.*\n### lets start..","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json        # deals with json files\nimport seaborn as sns  #visualization library\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-09T17:04:57.905550Z","iopub.execute_input":"2021-10-09T17:04:57.905814Z","iopub.status.idle":"2021-10-09T17:04:57.917019Z","shell.execute_reply.started":"2021-10-09T17:04:57.905789Z","shell.execute_reply":"2021-10-09T17:04:57.916143Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/news-category-dataset/News_Category_Dataset_v2.json\",mode='r') as json_file :\n    List_of_dict=[ json.loads(line) for line in json_file ]    \n        \ndf=pd.DataFrame(List_of_dict)\ndf.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:04:57.919054Z","iopub.execute_input":"2021-10-09T17:04:57.919771Z","iopub.status.idle":"2021-10-09T17:05:00.429356Z","shell.execute_reply.started":"2021-10-09T17:04:57.919739Z","shell.execute_reply":"2021-10-09T17:05:00.428542Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(df.dtypes)\n#here date needs to be in datetime format.Lets do it now\n\ndf.date= pd.to_datetime(df.date)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:00.430609Z","iopub.execute_input":"2021-10-09T17:05:00.430853Z","iopub.status.idle":"2021-10-09T17:05:00.583995Z","shell.execute_reply.started":"2021-10-09T17:05:00.430825Z","shell.execute_reply":"2021-10-09T17:05:00.583142Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# straight away we can see empty entries. So lets convert it to nans and handle them.\ndf.replace('',np.nan,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:00.585085Z","iopub.execute_input":"2021-10-09T17:05:00.585341Z","iopub.status.idle":"2021-10-09T17:05:00.656977Z","shell.execute_reply.started":"2021-10-09T17:05:00.585313Z","shell.execute_reply":"2021-10-09T17:05:00.656063Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum() \n# So short_description  and authors have most nan values","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:00.659033Z","iopub.execute_input":"2021-10-09T17:05:00.659267Z","iopub.status.idle":"2021-10-09T17:05:00.802306Z","shell.execute_reply.started":"2021-10-09T17:05:00.659240Z","shell.execute_reply":"2021-10-09T17:05:00.801416Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Short_description is an important feature and contains nan values. So dropping nans present in it\ndf.dropna(axis=0,how='all',subset=['short_description'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:00.803768Z","iopub.execute_input":"2021-10-09T17:05:00.804115Z","iopub.status.idle":"2021-10-09T17:05:00.965101Z","shell.execute_reply.started":"2021-10-09T17:05:00.804075Z","shell.execute_reply":"2021-10-09T17:05:00.964225Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(8,8))\nsns.countplot(df.category)\nplt.xticks(rotation=90)\nfig.suptitle('Number of articles( for each category)')\n\n\n# politics and wellness seem to have most number of articles","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:00.966284Z","iopub.execute_input":"2021-10-09T17:05:00.966527Z","iopub.status.idle":"2021-10-09T17:05:02.041568Z","shell.execute_reply.started":"2021-10-09T17:05:00.966492Z","shell.execute_reply":"2021-10-09T17:05:02.040899Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"date_by_category=df.groupby(['date'])['category'].count().reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:02.042603Z","iopub.execute_input":"2021-10-09T17:05:02.042993Z","iopub.status.idle":"2021-10-09T17:05:02.077326Z","shell.execute_reply.started":"2021-10-09T17:05:02.042964Z","shell.execute_reply":"2021-10-09T17:05:02.076514Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"year=date_by_category.date.apply(lambda x: datetime.date(x).year)\narticles_per_year=year.value_counts().reset_index()\narticles_per_year.columns=['year','number_of articles']\nsns.barplot(articles_per_year.year,articles_per_year['number_of articles'],data=articles_per_year)\n\n# year(2013-2017 had most articles) while 2018 had the least articles","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:02.078516Z","iopub.execute_input":"2021-10-09T17:05:02.078943Z","iopub.status.idle":"2021-10-09T17:05:02.343198Z","shell.execute_reply.started":"2021-10-09T17:05:02.078900Z","shell.execute_reply":"2021-10-09T17:05:02.342281Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"month= date_by_category.date.apply(lambda x: datetime.date(x).month)\narticles_per_month=month.value_counts().reset_index()\narticles_per_month.columns=['month_num','number_of articles']\nsns.barplot(articles_per_month['month_num'],articles_per_month['number_of articles'],data=articles_per_month)\n\n# march & may had most while june had least number of articles","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:02.344491Z","iopub.execute_input":"2021-10-09T17:05:02.344902Z","iopub.status.idle":"2021-10-09T17:05:02.646285Z","shell.execute_reply.started":"2021-10-09T17:05:02.344862Z","shell.execute_reply":"2021-10-09T17:05:02.645415Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df.drop(['authors','date'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:02.647449Z","iopub.execute_input":"2021-10-09T17:05:02.647655Z","iopub.status.idle":"2021-10-09T17:05:02.683087Z","shell.execute_reply.started":"2021-10-09T17:05:02.647631Z","shell.execute_reply":"2021-10-09T17:05:02.682083Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## dropping date (its not helpful identifying the category of an article) and authors (it contains names and designation which are not required)\n","metadata":{}},{"cell_type":"code","source":"df.link[52]","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:02.684369Z","iopub.execute_input":"2021-10-09T17:05:02.684775Z","iopub.status.idle":"2021-10-09T17:05:02.696256Z","shell.execute_reply.started":"2021-10-09T17:05:02.684732Z","shell.execute_reply":"2021-10-09T17:05:02.694003Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### * I think i saw something useful in df [\"link\"].\n### * If i can extract the keywords in a url then it would be great for nlp. \n### * For example= 'https://www.huffingtonpost.com/entry/hollywood-doesnt-need-difficult-men-to-make-great-tv_us_5b080bcce4b0568a880aa6d9'. \n### * In this example \"hollywood-doesnt-need-difficult-men-to-make-great-tv\" is a keyword which helps to represent Entertainment category.\n### lets go..","metadata":{}},{"cell_type":"code","source":"df=df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:02.697837Z","iopub.execute_input":"2021-10-09T17:05:02.698395Z","iopub.status.idle":"2021-10-09T17:05:02.728805Z","shell.execute_reply.started":"2021-10-09T17:05:02.698314Z","shell.execute_reply":"2021-10-09T17:05:02.728021Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# for each record in df.link we split it on '_' then split on 'y/' to get the exact set of keywords\n# then we pass records which find 'http'(2 times) in a single url\n\ndf['keywords']=''\ntry:\n    for i in range(len(df.link)):\n        if df['link'][i].count(\"http\")>=2:\n            pass\n            \n        else:\n            df['keywords'][i]= df['link'][i].split('_')[0].split('y/')[1]\n\nexcept IndexError:\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:05:02.731966Z","iopub.execute_input":"2021-10-09T17:05:02.732639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This step is optional: I want to make sure that each label in our target column(category) has equal number of corresponding reccords.\n### example: Politics has 29k records but entertainment has 13k records\n### by doing this step we will eliminate imbalanced category","metadata":{}},{"cell_type":"code","source":"# I am going to take 5k random reccords from 6 category (politics, wellness, food, sports, business,world news)\n\n#worldpost and theworldpost belongs to world news\ndf.category = df.category.map(lambda x: \"WORLD NEWS\" if x == \"THE WORLDPOST\" or x==\"WORLDPOST\" else x) \n\n\ndf_politics=df.loc[df['category']=='POLITICS'].sample(5000)\ndf_wellness= df.loc[df['category']=='WELLNESS'].sample(5000)\ndf_food= df.loc[df['category']=='FOOD & DRINK'].sample(5000)\ndf_world_news=df.loc[df['category']==\"WORLD NEWS\"].sample(5000)\ndf_parenting=df.loc[df['category']=='PARENTING'].sample(5000)\ndf_business=df.loc[df['category']=='BUSINESS'].sample(5000,replace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('final_news_df.csv',index=False) #exporting to csv file","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Thanks for tuning in. We will continue the visualization in the next notebook. Hope it helped my fellow beginners !!\n\n#### Also  I have cleaned up the new csv file a little more. Checkout the dataset: [https://www.kaggle.com/setseries/news-category-dataset]  ","metadata":{}}]}