{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### *In this notebook we are going to do visualization for news category dataset. Hope it will help fellow beginners in getting started with data visualization.*\n\n## lets start..","metadata":{"editable":false}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk import tokenize\nfrom tensorflow.keras.preprocessing.text import Tokenizer,  text_to_word_sequence\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:40:07.698193Z","iopub.execute_input":"2021-10-09T15:40:07.698478Z","iopub.status.idle":"2021-10-09T15:40:07.708159Z","shell.execute_reply.started":"2021-10-09T15:40:07.698449Z","shell.execute_reply":"2021-10-09T15:40:07.707361Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/news-category-dataset/final_news_df.csv\")\ndf.sample(4)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:40:08.827122Z","iopub.execute_input":"2021-10-09T15:40:08.827872Z","iopub.status.idle":"2021-10-09T15:40:09.005767Z","shell.execute_reply.started":"2021-10-09T15:40:08.827838Z","shell.execute_reply":"2021-10-09T15:40:09.004840Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df['text']= df['headline'] + df['short_description']+ df['keywords']\ndf.drop(['headline','short_description','date','keywords'],axis=1,inplace=True)\ndf.dropna(how='all',axis=0,subset=['text'],inplace=True)\ndf= df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:41:43.978526Z","iopub.execute_input":"2021-10-09T15:41:43.978806Z","iopub.status.idle":"2021-10-09T15:41:44.040840Z","shell.execute_reply.started":"2021-10-09T15:41:43.978772Z","shell.execute_reply":"2021-10-09T15:41:44.040113Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Plot each article's length","metadata":{"editable":false}},{"cell_type":"code","source":"df['News_length']= df.text.str.len()   #this gives length of each news article\ndf.dropna(axis=0,how='all',subset=['text','News_length'],inplace=True) #dropping all nans in text column\ndf['News_length']=df['News_length'].astype(int)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:41:51.367390Z","iopub.execute_input":"2021-10-09T15:41:51.367667Z","iopub.status.idle":"2021-10-09T15:41:51.408389Z","shell.execute_reply.started":"2021-10-09T15:41:51.367641Z","shell.execute_reply":"2021-10-09T15:41:51.407531Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"sns.displot(df['News_length'])\n# most articles have 200 words","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:41:53.643371Z","iopub.execute_input":"2021-10-09T15:41:53.643637Z","iopub.status.idle":"2021-10-09T15:41:54.463924Z","shell.execute_reply.started":"2021-10-09T15:41:53.643611Z","shell.execute_reply":"2021-10-09T15:41:54.463009Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Plot number and length of each sentence in each article","metadata":{"editable":false}},{"cell_type":"code","source":"df=df.reset_index(drop=True)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:41:55.022695Z","iopub.execute_input":"2021-10-09T15:41:55.022998Z","iopub.status.idle":"2021-10-09T15:41:55.028857Z","shell.execute_reply.started":"2021-10-09T15:41:55.022969Z","shell.execute_reply":"2021-10-09T15:41:55.028175Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df.text=df.text.apply(lambda x: x.strip().lower())   # removing empty spaces and converting text to lowercase","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:41:55.673498Z","iopub.execute_input":"2021-10-09T15:41:55.674256Z","iopub.status.idle":"2021-10-09T15:41:55.701653Z","shell.execute_reply.started":"2021-10-09T15:41:55.674211Z","shell.execute_reply":"2021-10-09T15:41:55.700972Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"sentence_len=[]     #here we will store length of each sentence in each article\nsentence_num=[]    #here we will store number of sentences in each article\nall_texts=[]       #here all the tokenized sentences will go \n\nfor index in range(df.text.shape[0]):\n    sentences=tokenize.sent_tokenize(df.text[index])\n    sentence_num.append(len(sentences))\n    \n    for sentence in sentences:\n        sentence_len.append(len(text_to_word_sequence(sentence)))\n        \n    all_texts.append(sentences)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:43:29.585029Z","iopub.execute_input":"2021-10-09T15:43:29.585818Z","iopub.status.idle":"2021-10-09T15:43:33.982854Z","shell.execute_reply.started":"2021-10-09T15:43:29.585772Z","shell.execute_reply":"2021-10-09T15:43:33.982109Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"sns.distplot(sentence_len,bins=200)     # we have more sentences of shorter length","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:43:33.984344Z","iopub.execute_input":"2021-10-09T15:43:33.984645Z","iopub.status.idle":"2021-10-09T15:43:34.936863Z","shell.execute_reply.started":"2021-10-09T15:43:33.984613Z","shell.execute_reply":"2021-10-09T15:43:34.936091Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"sns.distplot(sentence_num)  # in each record sentence-number(2) has more words than other sentences ","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:43:35.664704Z","iopub.execute_input":"2021-10-09T15:43:35.664980Z","iopub.status.idle":"2021-10-09T15:43:36.180274Z","shell.execute_reply.started":"2021-10-09T15:43:35.664952Z","shell.execute_reply":"2021-10-09T15:43:36.179501Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Now we perform nlp operations to get a cleaner data","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import  stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\n\ndef clean_texts(df):\n    lemmatizer=WordNetLemmatizer()\n    corpus=[]\n    for i in range(0,len(df.text)):\n        review=re.sub('[^a-zA-Z]',' ',df.text[i])\n        review=review.lower().split()\n            \n        review=[lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n        review=' '.join(review)\n        corpus.append(review)\n    df['text']= corpus\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:43:47.659893Z","iopub.execute_input":"2021-10-09T15:43:47.660182Z","iopub.status.idle":"2021-10-09T15:43:47.667280Z","shell.execute_reply.started":"2021-10-09T15:43:47.660150Z","shell.execute_reply":"2021-10-09T15:43:47.666481Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df1=clean_texts(df)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:43:48.918478Z","iopub.execute_input":"2021-10-09T15:43:48.918778Z","iopub.status.idle":"2021-10-09T15:46:06.796513Z","shell.execute_reply.started":"2021-10-09T15:43:48.918748Z","shell.execute_reply":"2021-10-09T15:46:06.795357Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Word Cloud","metadata":{"editable":false}},{"cell_type":"code","source":"from wordcloud import WordCloud","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:46:06.798185Z","iopub.execute_input":"2021-10-09T15:46:06.798448Z","iopub.status.idle":"2021-10-09T15:46:06.842938Z","shell.execute_reply.started":"2021-10-09T15:46:06.798420Z","shell.execute_reply":"2021-10-09T15:46:06.842121Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def wordcloud_generator(words):\n    wc= WordCloud(width=800,height=600,random_state=42,max_font_size=100).generate(words)\n    plt.figure(figsize=(8,8))\n    plt.imshow(wc,interpolation='bilinear')\n    plt.show()\n    \n\ndef word_category(category):\n    subset=df1[df1['category']==category]\n    texts=subset.text.values\n    words= ''.join(texts)\n    wordcloud_generator(words)\n    \nword_category('POLITICS') #enter here the category ","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:46:06.844020Z","iopub.execute_input":"2021-10-09T15:46:06.844260Z","iopub.status.idle":"2021-10-09T15:46:09.251421Z","shell.execute_reply.started":"2021-10-09T15:46:06.844221Z","shell.execute_reply":"2021-10-09T15:46:09.249018Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Now we make few more columns for wordcount, character count and avg_word_length","metadata":{}},{"cell_type":"code","source":"df1[\"wordcount\"]= df1.text.apply(lambda x: len(x.split()))\ndf1['char_count']= df1.text.apply(lambda x:sum (len(word) for word in x.split()))\ndf1['avg_word_len']= round(df1.char_count/df1.wordcount,2)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:47:45.355906Z","iopub.execute_input":"2021-10-09T15:47:45.356219Z","iopub.status.idle":"2021-10-09T15:47:45.579014Z","shell.execute_reply.started":"2021-10-09T15:47:45.356189Z","shell.execute_reply":"2021-10-09T15:47:45.578355Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"sns.set_theme( palette=\"husl\")\nfig,ax= plt.subplots(nrows=1,ncols=2,figsize=(10,6))\n\nfor i in df1.category.unique():\n    sns.distplot(df1[df1.category==i]['char_count'],kde=False,bins=10,ax=ax[0])\n    sns.distplot(df1[df1.category==i]['char_count'],kde=True,ax=ax[1])\n    \n\nax[0].legend(df1.category.unique())\nax[1].legend(df1.category.unique())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:47:45.848860Z","iopub.execute_input":"2021-10-09T15:47:45.849140Z","iopub.status.idle":"2021-10-09T15:47:47.751654Z","shell.execute_reply.started":"2021-10-09T15:47:45.849111Z","shell.execute_reply":"2021-10-09T15:47:47.748930Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Lets use textblob to understand polarity of sentiment in each text record","metadata":{}},{"cell_type":"raw","source":"# !pip install -U textblob","metadata":{"execution":{"iopub.status.busy":"2021-10-05T08:58:09.15937Z","iopub.execute_input":"2021-10-05T08:58:09.159698Z"},"editable":false}},{"cell_type":"code","source":"from textblob import TextBlob","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:47:49.949444Z","iopub.execute_input":"2021-10-09T15:47:49.949714Z","iopub.status.idle":"2021-10-09T15:47:49.996595Z","shell.execute_reply.started":"2021-10-09T15:47:49.949687Z","shell.execute_reply":"2021-10-09T15:47:49.995800Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df1['sentiment']=df1.text.apply(lambda x:TextBlob(x).sentiment.polarity)\ndf1.sentiment= np.round(df1.sentiment,2)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:47:52.168005Z","iopub.execute_input":"2021-10-09T15:47:52.168693Z","iopub.status.idle":"2021-10-09T15:48:00.209847Z","shell.execute_reply.started":"2021-10-09T15:47:52.168652Z","shell.execute_reply":"2021-10-09T15:48:00.208970Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"sns.set_theme( palette=\"husl\")\nfig,ax= plt.subplots(nrows=1,ncols=2,figsize=(10,6))\nfig.suptitle('Sentiment polarity in different categories',fontsize=10)\n\n\nfor i in df1.category.unique():\n    sns.distplot(df1[df1[\"category\"]==i]['sentiment'],kde=False,hist=True,ax=ax[0])\n    sns.distplot(df1[df1[\"category\"]==i][\"sentiment\"],kde=True,hist=False,ax=ax[1])\n    \nax[0].legend(df1.category.unique())\nax[1].legend(df1.category.unique())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-10-09T15:48:00.211385Z","iopub.execute_input":"2021-10-09T15:48:00.211600Z","iopub.status.idle":"2021-10-09T15:48:02.053127Z","shell.execute_reply.started":"2021-10-09T15:48:00.211575Z","shell.execute_reply":"2021-10-09T15:48:02.052308Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df1.loc[1,'text']","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:47:46.880919Z","iopub.execute_input":"2021-10-05T16:47:46.881315Z","iopub.status.idle":"2021-10-05T16:47:46.891689Z","shell.execute_reply.started":"2021-10-05T16:47:46.881282Z","shell.execute_reply":"2021-10-05T16:47:46.890568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Name Entity Recognizer","metadata":{}},{"cell_type":"code","source":"import spacy\nner=spacy.load(\"en_core_web_lg\")\ntext='RIM CEO Thorsten Heins Significant Plans For a Party.'\ndoc=ner(text).ents\nspacy.displacy.render(doc,style='ent')","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:48:31.122007Z","iopub.execute_input":"2021-10-09T15:48:31.122331Z","iopub.status.idle":"2021-10-09T15:48:41.291442Z","shell.execute_reply.started":"2021-10-09T15:48:31.122298Z","shell.execute_reply":"2021-10-09T15:48:41.290625Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## **IMPORTANT NOTE**:\n### RIM==> tag ,  ORG==>tag_type \n### Thorsten Heins==>tag, PERSON==>tag_type","metadata":{}},{"cell_type":"code","source":"df1['tags']=df1.text.apply(lambda x: [(tag.text,tag.label_) for tag in ner(x).ents])","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:52:17.217792Z","iopub.execute_input":"2021-10-09T15:52:17.218703Z","iopub.status.idle":"2021-10-09T15:57:16.635892Z","shell.execute_reply.started":"2021-10-09T15:52:17.218652Z","shell.execute_reply":"2021-10-09T15:57:16.634843Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Now we will have a list of tags and tag_names in a separate columns","metadata":{}},{"cell_type":"markdown","source":"# In next few cells we will count eachtime a tag or tag_type occurs and join with our dataframe","metadata":{}},{"cell_type":"code","source":"import collections \ndef utils_lst_count(lst):\n    dic_counter = collections.Counter()\n    for x in lst:\n        dic_counter[x] += 1\n\n    dic_counter = collections.OrderedDict( \n                     sorted(dic_counter.items(), \n                     key=lambda x: x[1], reverse=True))\n    lst_count = [ {key:value} for key,value in dic_counter.items() ]\n    return lst_count","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:57:16.638185Z","iopub.execute_input":"2021-10-09T15:57:16.638501Z","iopub.status.idle":"2021-10-09T15:57:16.645635Z","shell.execute_reply.started":"2021-10-09T15:57:16.638455Z","shell.execute_reply":"2021-10-09T15:57:16.644814Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"df1.tags=df1.tags.apply(lambda x: utils_lst_count(x))","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:34:17.219684Z","iopub.execute_input":"2021-10-09T16:34:17.219967Z","iopub.status.idle":"2021-10-09T16:34:17.223445Z","shell.execute_reply.started":"2021-10-09T16:34:17.219934Z","shell.execute_reply":"2021-10-09T16:34:17.222648Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def utils_ner_features(lst_dics_tuples, tag):\n    if len(lst_dics_tuples) > 0:\n        tag_type = []\n        for dic_tuples in lst_dics_tuples:\n            for tuples in dic_tuples:\n                types, n = tuples[1], dic_tuples[tuples]\n                tag_type = tag_type + [types]*n\n                dic_counter = collections.Counter()\n                for x in tag_type:\n                    dic_counter[x] += 1\n        return dic_counter[tag]\n    else:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:57:17.411764Z","iopub.execute_input":"2021-10-09T15:57:17.411987Z","iopub.status.idle":"2021-10-09T15:57:17.418550Z","shell.execute_reply.started":"2021-10-09T15:57:17.411963Z","shell.execute_reply":"2021-10-09T15:57:17.417423Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### We will add a column for each tag_type  and then increment it each time a tag is found in the corresponding row","metadata":{}},{"cell_type":"code","source":"tags_set = []\nfor lst in df1[\"tags\"].tolist():\n    for dic in lst:\n        for k in dic.keys():\n              tags_set.append(k[1])\ntags_set = list(set(tags_set))\nfor feature in tags_set:\n    df1[\"tags_\"+feature] = df1[\"tags\"].apply(lambda x:utils_ner_features(x, feature))                      ","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:57:17.419791Z","iopub.execute_input":"2021-10-09T15:57:17.420012Z","iopub.status.idle":"2021-10-09T15:57:22.066949Z","shell.execute_reply.started":"2021-10-09T15:57:17.419985Z","shell.execute_reply":"2021-10-09T15:57:22.065990Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Here we plot summation of each tag_type count ","metadata":{}},{"cell_type":"code","source":"sns.barplot(data=df1.iloc[:,7:])\nplt.xticks(rotation=90)\n\n#  most used tag_type are 'person' followed by 'org'","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:57:22.068334Z","iopub.execute_input":"2021-10-09T15:57:22.068563Z","iopub.status.idle":"2021-10-09T15:57:25.855441Z","shell.execute_reply.started":"2021-10-09T15:57:22.068534Z","shell.execute_reply":"2021-10-09T15:57:25.854573Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Now plotting most famous tags in a specific news category","metadata":{}},{"cell_type":"code","source":"def popular_tags(category):\n    tags_list = df1[df1[\"category\"]==category][\"tags\"].sum()\n    map_lst = list(map(lambda x: list(x.keys())[0], tags_list))\n    dtf_tags = pd.DataFrame(map_lst, columns=['tag','type'])\n    dtf_tags[\"count\"] = 1\n    dtf_tags = dtf_tags.groupby(['type',  \n                    'tag']).count().reset_index().sort_values(\"count\", \n                     ascending=False)\n    return dtf_tags\n\n\ndef plot_popular_tags():\n    dtf_tags=popular_tags('WORLD NEWS')\n    fig, ax = plt.subplots()\n    fig.suptitle(\"Top frequent tags\", fontsize=12)\n    sns.barplot(x=\"count\", y=\"tag\", hue=\"type\", \n                data=dtf_tags.iloc[:10,:], dodge=False, ax=ax)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:57:25.856880Z","iopub.execute_input":"2021-10-09T15:57:25.857137Z","iopub.status.idle":"2021-10-09T15:57:25.866129Z","shell.execute_reply.started":"2021-10-09T15:57:25.857108Z","shell.execute_reply":"2021-10-09T15:57:25.865112Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"popular_tags('BUSINESS')  #place a category here\n\n# one is the most popular word/tag in Business category","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:57:25.867437Z","iopub.execute_input":"2021-10-09T15:57:25.867680Z","iopub.status.idle":"2021-10-09T15:57:25.989709Z","shell.execute_reply.started":"2021-10-09T15:57:25.867653Z","shell.execute_reply":"2021-10-09T15:57:25.988721Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Lets do something similar with Bigrams and Trigrams","metadata":{}},{"cell_type":"code","source":"import nltk\ncorpus=df1[df1[\"category\"]=='POLITICS']['text']\nlst_tokens= tokenize.word_tokenize(corpus.str.cat(sep=' '))\nfig, ax = plt.subplots(nrows=2, ncols=1)\nfig.suptitle(\"Most frequent words\", fontsize=15)\n    \n## unigrams\ndic_words_freq = nltk.FreqDist(lst_tokens)\ndtf_uni = pd.DataFrame(dic_words_freq.most_common(), \n                       columns=[\"Word\",\"Freq\"])\ndtf_uni.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n                  kind=\"barh\", title=\"Unigrams\", ax=ax[0], \n                  legend=False).grid(axis='x')\n\n## bigrams\ndic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens,2))\ndtf_uni = pd.DataFrame(dic_words_freq.most_common(), \n                       columns=[\"Word\",\"Freq\"])\ndtf_uni['Word']= dtf_uni['Word'].apply(lambda x: ' '.join(x))\ndtf_uni.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n                  kind=\"barh\", title=\"Bigrams\", ax=ax[1], \n                  legend=False).grid(axis='x')\nplt.yticks(fontsize=8,rotation=0)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:57:25.990757Z","iopub.execute_input":"2021-10-09T15:57:25.990981Z","iopub.status.idle":"2021-10-09T15:57:27.951792Z","shell.execute_reply.started":"2021-10-09T15:57:25.990957Z","shell.execute_reply":"2021-10-09T15:57:27.951024Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}